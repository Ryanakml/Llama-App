{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLAMA MODEL FROM SCARTCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Dependecies\n",
    "\n",
    "1. tocrh as matrix operations\n",
    "2. torch.nn as neural network operations like linear layers, activation functions, etc.\n",
    "3. torch.nn.functional as functional operations like loss functions, activation functions, etc.\n",
    "4. torch.optim as optimizer functions like SGD, Adam, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RMSNorm\n",
    "\n",
    "- basically like norm layer but without mean subtraction\n",
    "- ![](https://pbs.twimg.com/media/GCRiqC6aIAAAh1M.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True))\n",
    "        x_norm = x / (rms * self.eps)\n",
    "        return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " tensor([[0.3969, 0.1416, 0.3920, 0.6198],\n",
      "        [0.5161, 0.8649, 0.1807, 0.9737]])\n",
      "Output setelah RMSNorm:\n",
      " tensor([[ 938611.4375,  334802.7500,  926952.6250, 1465494.6250],\n",
      "        [ 730734.1250, 1224642.2500,  255796.4844, 1378712.1250]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,4)\n",
    "norm = RMSNorm(dim=4)\n",
    "output = norm(x)\n",
    "\n",
    "print(\"Input:\\n\", x)\n",
    "print(\"Output setelah RMSNorm:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotary Position Embedding (RoPE)\n",
    "\n",
    "Process : \n",
    "1. Compute sinusiodal freqs,\n",
    "2. cache sin and cos\n",
    "3. separate embedding into pairs\n",
    "4. rotate with sin and cos\n",
    "5. combine and return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (16) must match the size of tensor b (32) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     76\u001b[39m mha = MultiHeadAttention(config)\n\u001b[32m     77\u001b[39m input_tensor = torch.randn(\u001b[32m2\u001b[39m, \u001b[32m5\u001b[39m, config[\u001b[33m\"\u001b[39m\u001b[33mhidden_size\u001b[39m\u001b[33m\"\u001b[39m])  \u001b[38;5;66;03m# (batch, seq_len, hidden)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m output = \u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOutput shape:\u001b[39m\u001b[33m\"\u001b[39m, output.shape)  \u001b[38;5;66;03m# seharusnya (2, 5, 256)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/[1] BS/[2] Freelance/[1] EMERGING/[2] AI/[6] LLM/LLAMA - RYAN MODELS/myenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/[1] BS/[2] Freelance/[1] EMERGING/[2] AI/[6] LLM/LLAMA - RYAN MODELS/myenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, cache)\u001b[39m\n\u001b[32m     50\u001b[39m key = key.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Apply Rotary Positional Embedding\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m query = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m key = \u001b[38;5;28mself\u001b[39m.rotary_emb(key)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Transpose kembali\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/[1] BS/[2] Freelance/[1] EMERGING/[2] AI/[6] LLM/LLAMA - RYAN MODELS/myenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/[1] BS/[2] Freelance/[1] EMERGING/[2] AI/[6] LLM/LLAMA - RYAN MODELS/myenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mRotaryEmbedding.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     24\u001b[39m x1 = x[..., ::\u001b[32m2\u001b[39m]\n\u001b[32m     25\u001b[39m x2 = x[..., \u001b[32m1\u001b[39m::\u001b[32m2\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m x_rotated = torch.cat([\u001b[43mx1\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m - x2 * sin, x1 * sin + x2 * cos], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x_rotated\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (16) must match the size of tensor b (32) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_position_embeddings, dtype=torch.float)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)  # (seq_len, dim)\n",
    "\n",
    "        self.cos_cached = emb.cos()[None, :, None, :]\n",
    "        self.sin_cached = emb.sin()[None, :, None, :]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_len, batch, head, dim)\n",
    "        seq_len = x.size(0)\n",
    "        cos = self.cos_cached[:, :seq_len, :, :].to(x.device)  # (1, seq_len, 1, dim)\n",
    "        sin = self.sin_cached[:, :seq_len, :, :].to(x.device)  # (1, seq_len, 1, dim)\n",
    "\n",
    "        # (seq_len, batch, head, dim)\n",
    "        x1 = x[..., ::2]\n",
    "        x2 = x[..., 1::2]\n",
    "        x_rotated = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "        return x_rotated\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_heads = config[\"num_attention_heads\"]\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.hidden_size, 3 * self.hidden_size)\n",
    "        self.out_proj = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, cache=None):\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "\n",
    "        qkv = self.qkv_proj(hidden_states)  # (batch, seq_len, 3 * hidden_size)\n",
    "        qkv = qkv.view(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        query, key, value = qkv.unbind(dim=2)  # masing-masing (batch, seq_len, num_heads, head_dim)\n",
    "\n",
    "        # Transpose untuk RoPE\n",
    "        query = query.transpose(0, 1)  # (seq_len, batch, num_heads, head_dim)\n",
    "        key = key.transpose(0, 1)\n",
    "\n",
    "        # Apply Rotary Positional Embedding\n",
    "        query = self.rotary_emb(query)\n",
    "        key = self.rotary_emb(key)\n",
    "\n",
    "        # Transpose kembali\n",
    "        query = query.transpose(0, 1)  # (batch, seq_len, num_heads, head_dim)\n",
    "        key = key.transpose(0, 1)\n",
    "\n",
    "        # Attention score computation\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if attention_mask is not None:\n",
    "            scores = scores + attention_mask\n",
    "\n",
    "        attn_probs = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn_probs, value)  # (batch, seq_len, num_heads, head_dim)\n",
    "\n",
    "        context = context.transpose(1, 2).reshape(batch_size, seq_len, self.hidden_size)\n",
    "        output = self.out_proj(context)\n",
    "        return output\n",
    "\n",
    "config = {\n",
    "    \"hidden_size\": 256,\n",
    "    \"num_attention_heads\": 8,\n",
    "}\n",
    "mha = MultiHeadAttention(config)\n",
    "input_tensor = torch.randn(2, 5, config[\"hidden_size\"])  # (batch, seq_len, hidden)\n",
    "output = mha(input_tensor)\n",
    "print(\"Output shape:\", output.shape)  # seharusnya (2, 5, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RotaryEmbedding.__init__() got an unexpected keyword argument 'max_seq_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m batch, seq_len, heads, dim = \u001b[32m1\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m8\u001b[39m  \n\u001b[32m      2\u001b[39m x = torch.randn(batch, seq_len, heads, dim)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m rope = \u001b[43mRotaryEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m x_rope = rope(x, seq_len=seq_len)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInput shape:\u001b[39m\u001b[33m\"\u001b[39m, x.shape)\n",
      "\u001b[31mTypeError\u001b[39m: RotaryEmbedding.__init__() got an unexpected keyword argument 'max_seq_len'"
     ]
    }
   ],
   "source": [
    "batch, seq_len, heads, dim = 1, 5, 2, 8  \n",
    "x = torch.randn(batch, seq_len, heads, dim)\n",
    "rope = RotaryEmbedding(dim=dim, max_seq_len=10)\n",
    "x_rope = rope(x, seq_len=seq_len)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape setelah RoPE:\", x_rope.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swiglu Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SwiGlu(nn.Module):\n",
    "    def __init__(self, in_features: int, hidden_features: int):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(in_features, hidden_features, bias=False)\n",
    "        self.up_proj = nn.Linear(in_features, hidden_features, bias=False)\n",
    "        self.down_proj = nn.Linear(hidden_features, in_features, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.gate_proj(x)\n",
    "        up = self.up_proj(x)\n",
    "        activated = F.silu(gate) * up\n",
    "        return self.down_proj(activated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output setelah SwiGLU:\n",
      " tensor([[-0.0308, -0.0324,  0.0450,  0.0164],\n",
      "        [ 0.0333, -0.0289, -0.0031,  0.0062]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Tes SwiGLU\n",
    "x = torch.randn(2, 4)\n",
    "swiglu = SwiGlu(in_features=4, hidden_features=8)\n",
    "output = swiglu(x)\n",
    "print(\"Output setelah SwiGLU:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for training and inference the parameter, we can do this\n",
    "\n",
    "```python\n",
    "# Dummy input and target\n",
    "x = torch.rand(2, 4, 4)\n",
    "target = torch.rand(2, 4, 4)\n",
    "\n",
    "# Model\n",
    "model = SwiGlu(in_features=4, hidden_features=8)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training step\n",
    "output = model(x)\n",
    "loss = criterion(output, target)\n",
    "\n",
    "optimizer.zero_grad()       # Clear previous gradients\n",
    "loss.backward()             # Backpropagation\n",
    "optimizer.step()            # Update weights\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHead Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_heads = config[\"num_attention_heads\"]\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(self.hidden_size, 3 * self.hidden_size)\n",
    "        self.out_proj = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, cache=None):\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "\n",
    "        qkv = self.qkv_proj(hidden_states)  # (batch, seq_len, 3 * hidden_size)\n",
    "        qkv = qkv.view(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        query, key, value = qkv.unbind(dim=2)  # masing-masing (batch, seq_len, num_heads, head_dim)\n",
    "\n",
    "        # Transpose untuk RoPE\n",
    "        query = query.transpose(0, 1)  # (seq_len, batch, num_heads, head_dim)\n",
    "        key = key.transpose(0, 1)\n",
    "\n",
    "        # Apply Rotary Positional Embedding\n",
    "        query = self.rotary_emb(query)\n",
    "        key = self.rotary_emb(key)\n",
    "\n",
    "        # Transpose kembali\n",
    "        query = query.transpose(0, 1)  # (batch, seq_len, num_heads, head_dim)\n",
    "        key = key.transpose(0, 1)\n",
    "\n",
    "        # Attention score computation\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if attention_mask is not None:\n",
    "            scores = scores + attention_mask\n",
    "\n",
    "        attn_probs = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn_probs, value)  # (batch, seq_len, num_heads, head_dim)\n",
    "\n",
    "        context = context.transpose(1, 2).reshape(batch_size, seq_len, self.hidden_size)\n",
    "        output = self.out_proj(context)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (16) must match the size of tensor b (32) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m mha = MultiHeadAttention(config)\n\u001b[32m      6\u001b[39m input_tensor = torch.randn(\u001b[32m2\u001b[39m, \u001b[32m5\u001b[39m, config[\u001b[33m\"\u001b[39m\u001b[33mhidden_size\u001b[39m\u001b[33m\"\u001b[39m])  \u001b[38;5;66;03m# (batch, seq_len, hidden)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m output = \u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOutput shape:\u001b[39m\u001b[33m\"\u001b[39m, output.shape)  \u001b[38;5;66;03m# seharusnya (2, 5, 256)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/[1] BS/[2] Freelance/[1] EMERGING/[2] AI/[6] LLM/LLAMA - RYAN MODELS/myenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/[1] BS/[2] Freelance/[1] EMERGING/[2] AI/[6] LLM/LLAMA - RYAN MODELS/myenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, cache)\u001b[39m\n\u001b[32m     22\u001b[39m key = key.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Apply Rotary Positional Embedding\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m query = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m key = \u001b[38;5;28mself\u001b[39m.rotary_emb(key)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Transpose kembali\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/[1] BS/[2] Freelance/[1] EMERGING/[2] AI/[6] LLM/LLAMA - RYAN MODELS/myenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/[1] BS/[2] Freelance/[1] EMERGING/[2] AI/[6] LLM/LLAMA - RYAN MODELS/myenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mRotaryEmbedding.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     24\u001b[39m x1 = x[..., ::\u001b[32m2\u001b[39m]\n\u001b[32m     25\u001b[39m x2 = x[..., \u001b[32m1\u001b[39m::\u001b[32m2\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m x_rotated = torch.cat([\u001b[43mx1\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m - x2 * sin, x1 * sin + x2 * cos], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x_rotated\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (16) must match the size of tensor b (32) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"hidden_size\": 256,\n",
    "    \"num_attention_heads\": 8,\n",
    "}\n",
    "mha = MultiHeadAttention(config)\n",
    "input_tensor = torch.randn(2, 5, config[\"hidden_size\"])  # (batch, seq_len, hidden)\n",
    "output = mha(input_tensor)\n",
    "print(\"Output shape:\", output.shape)  # seharusnya (2, 5, 256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
