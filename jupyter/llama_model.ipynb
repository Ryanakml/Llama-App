{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLAMA MODEL FROM SCARTCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Dependecies\n",
    "\n",
    "1. tocrh as matrix operations\n",
    "2. torch.nn as neural network operations like linear layers, activation functions, etc.\n",
    "3. torch.nn.functional as functional operations like loss functions, activation functions, etc.\n",
    "4. torch.optim as optimizer functions like SGD, Adam, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RMSNorm\n",
    "\n",
    "- basically like norm layer but without mean subtraction\n",
    "- ![](https://pbs.twimg.com/media/GCRiqC6aIAAAh1M.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True))\n",
    "        x_norm = x / (rms * self.eps)\n",
    "        return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " tensor([[0.3969, 0.1416, 0.3920, 0.6198],\n",
      "        [0.5161, 0.8649, 0.1807, 0.9737]])\n",
      "Output setelah RMSNorm:\n",
      " tensor([[ 938611.4375,  334802.7500,  926952.6250, 1465494.6250],\n",
      "        [ 730734.1250, 1224642.2500,  255796.4844, 1378712.1250]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,4)\n",
    "norm = RMSNorm(dim=4)\n",
    "output = norm(x)\n",
    "\n",
    "print(\"Input:\\n\", x)\n",
    "print(\"Output setelah RMSNorm:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int = 2048):\n",
    "        \"\"\"\n",
    "        Process : \n",
    "        Compute sinusiodal freqs,\n",
    "        cache sin and cos\n",
    "        separate embedding into pairs\n",
    "        rotate with sin and cos\n",
    "        combine and return\n",
    "\n",
    "        Args:\n",
    "        dim: dimension of the embedding, must be even\n",
    "        max_seq_len: maximum sequence length, sentences\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # create freq for rope\n",
    "        freqs = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        position = torch.arange(max_seq_len).float()\n",
    "        freqs = torch.outer(position, freqs) # (max_seq_len, dim/2)\n",
    "\n",
    "        # cache cos and sin\n",
    "        self.register_buffer('cos_cache', torch.cos(freqs))\n",
    "        self.register_buffer('sin_cache', torch.sin(freqs))\n",
    "    \n",
    "    def forward(self, x, seq_len: int):\n",
    "        # x: (batch_size, seq_len, dim)\n",
    "        cos = self.cos_cache[:seq_len].view(seq_len, 1,1, self.dim//2)\n",
    "        sin = self.sin_cache[:seq_len].view(seq_len, 1,1, self.dim//2)\n",
    "\n",
    "        # reshape x for rotation\n",
    "        x_reshape = x.view(*x.shape[:-1], -1, 2)\n",
    "        x1, x2 = x_reshape[..., 0], x_reshape[..., 1]\n",
    "\n",
    "        # rotate\n",
    "        rotated_x1 = x1 * cos - x2 * sin\n",
    "        rotated_x2 = x1 * sin + x2 * cos\n",
    "\n",
    "        rotated_x = torch.stack([rotated_x1, rotated_x2], dim=-1)\n",
    "        return rotated_x.flatten(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 5, 2, 8])\n",
      "Output shape setelah RoPE: torch.Size([5, 5, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "batch, seq_len, heads, dim = 1, 5, 2, 8  \n",
    "x = torch.randn(batch, seq_len, heads, dim)\n",
    "rope = RotaryEmbedding(dim=dim, max_seq_len=10)\n",
    "x_rope = rope(x, seq_len=seq_len)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape setelah RoPE:\", x_rope.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SwiGlu(nn.Module):\n",
    "    def __init__(self, in_features: int, hidden_features: int):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(in_features, hidden_features, bias=False)\n",
    "        self.up_proj = nn.Linear(in_features, hidden_features, bias=False)\n",
    "        self.down_proj = nn.Linear(hidden_features, in_features, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.gate_proj(x)\n",
    "        up = self.up_proj(x)\n",
    "        activated = F.silu(gate) * up\n",
    "        return self.down_proj(activated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output setelah SwiGLU:\n",
      " tensor([[-0.0308, -0.0324,  0.0450,  0.0164],\n",
      "        [ 0.0333, -0.0289, -0.0031,  0.0062]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Tes SwiGLU\n",
    "x = torch.randn(2, 4)\n",
    "swiglu = SwiGlu(in_features=4, hidden_features=8)\n",
    "output = swiglu(x)\n",
    "print(\"Output setelah SwiGLU:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3757893741130829\n"
     ]
    }
   ],
   "source": [
    "# Dummy input and target\n",
    "x = torch.rand(2, 4, 4)\n",
    "target = torch.rand(2, 4, 4)\n",
    "\n",
    "# Model\n",
    "model = SwiGlu(in_features=4, hidden_features=8)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training step\n",
    "output = model(x)\n",
    "loss = criterion(output, target)\n",
    "\n",
    "optimizer.zero_grad()       # Clear previous gradients\n",
    "loss.backward()             # Backpropagation\n",
    "optimizer.step()            # Update weights\n",
    "\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
